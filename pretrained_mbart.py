# -*- coding: utf-8 -*-
"""Baseline_MBARTMMT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1od2fRIvn41dZ1aFNyL7U-rm4Uu1hj2Hv
"""

! pip install transformers torch datasets sacrebleu
! pip install unbabel-comet
! pip install tqdm

import pandas as pd
import gc
from transformers import MBartForConditionalGeneration, MBart50Tokenizer
from comet import download_model, load_from_checkpoint
from tqdm.auto import tqdm
import torch
from datasets import Dataset, load_metric
import numpy as np
import sacrebleu

"""# Preprocessing"""

def load_data(file_path, max_lines=None, chunksize=4000):
    with open(file_path, 'r', encoding='utf-8') as f:
        total_lines_read = 0
        while True:
            data = {'chinese': [], 'spanish': []}
            for _ in range(chunksize):
                line = f.readline()
                if not line:
                    break
                parts = line.strip().split('\t')
                if len(parts) < 2:
                    continue  # Skip incomplete lines
                zh_line, es_line = parts[0], parts[1]
                if max_lines and total_lines_read >= max_lines:
                    break
                data['chinese'].append(zh_line)
                data['spanish'].append(es_line)
                total_lines_read += 1
            if not data['chinese'] or not data['spanish']:
                break
            chunk = pd.DataFrame(data)
            chunk = chunk[chunk['chinese'].str.strip().astype(bool) & chunk['spanish'].str.strip().astype(bool)]
            yield Dataset.from_pandas(chunk)
            gc.collect()

# Function to preprocess and tokenize data, including setting labels
def preprocess_texts(inputs, targets, tokenizer, max_length=128):
    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding="max_length", return_tensors="pt")
    labels = tokenizer(targets, max_length=max_length, truncation=True, padding="max_length", return_tensors="pt")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Define the path to the test dataset
test_data_path = '/content/UNPC.test.zh-es'

test_dataset = load_data (test_data_path)

# Initialize lists to hold texts
input_texts = []
target_texts = []

for chunk in test_dataset:
    # Each 'chunk' is a Dataset object; extract 'chinese' and 'spanish' texts
    input_texts.extend(chunk['chinese'])
    target_texts.extend(chunk['spanish'])

"""# Load pre-trained model"""

def translate_texts(input_texts, tokenizer, model, device, batch_size=8, num_beams=5):
    translations = []
    total_batches = len(input_texts) // batch_size + (1 if len(input_texts) % batch_size != 0 else 0)
    for i in tqdm(range(0, len(input_texts), batch_size), total=total_batches, desc="Translating batches"):
        batch_texts = input_texts[i:i + batch_size]
        tokenized_inputs = tokenizer(batch_texts, max_length=128, truncation=True, padding="max_length", return_tensors="pt")
        tokenized_inputs = {k: v.to(device) for k, v in tokenized_inputs.items()}
        with torch.no_grad():
            translated_tokens = model.generate(
                **tokenized_inputs,
                forced_bos_token_id=tokenizer.lang_code_to_id["es_XX"],
                num_beams=num_beams,
                early_stopping=True
            )
        translations.extend(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True))

    return translations

# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize tokenizer and model
tokenizer = MBart50Tokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="zh_CN", tgt_lang="es_XX")
model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt").to(device)
model.to(device)

# Generate translations
predicted_texts = translate_texts(input_texts, tokenizer, model, device, batch_size=8, num_beams=5)

"""# Evaluation"""

# BLUE evaluation
metric = load_metric("sacrebleu")

# Preprocess the references to match the expected format for sacrebleu
references = [[ref] for ref in target_texts]

# Compute BLEU score
results = metric.compute(predictions=predicted_texts, references=references)
print(f"BLEU score: {results['score']}")

! python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/unbabel_comet/wmt20-comet-da/checkpoints/model.ckpt

# COMET evaluation
comet_model = load_from_checkpoint(download_model("wmt20-comet-da"))

comet_data = [{"src": src, "mt": mt, "ref": ref} for src, mt, ref in zip(input_texts, predicted_texts, target_texts)]
comet_score = comet_model.predict(comet_data, batch_size=16, gpus=1)
comet_mean_score = np.mean(comet_score['scores'])
print(f"COMET score: {comet_mean_score}")

# Save translations to CSV
df = pd.DataFrame({
    "Source Text": input_texts,
    "Machine Translation": predicted_texts,
    "Reference Text": target_texts
})

# Save the DataFrame to a CSV file
df.to_csv("translation_mBART50.csv", index=False)

print("Data saved to translation_evaluation.csv successfully.")
# -*- coding: utf-8 -*-
"""Google_translate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I4K9_Z-ERU1UGk6rsc6M5TpaOgM7QXCY
"""

! pip install requests
! pip install sacrebleu unbabel-comet
! pip install datasets

import requests
import sacrebleu
from comet import download_model, load_from_checkpoint
import pandas as pd
import gc
from datasets import Dataset, load_metric
import numpy as np

"""# Preprocessing"""

def load_data(file_path, max_lines=None, chunksize=4000):
    with open(file_path, 'r', encoding='utf-8') as f:
        total_lines_read = 0
        while True:
            data = {'chinese': [], 'spanish': []}
            for _ in range(chunksize):
                line = f.readline()
                if not line:
                    break
                parts = line.strip().split('\t')
                if len(parts) < 2:
                    continue  # Skip incomplete lines
                zh_line, es_line = parts[0], parts[1]
                if max_lines and total_lines_read >= max_lines:
                    break
                data['chinese'].append(zh_line)
                data['spanish'].append(es_line)
                total_lines_read += 1
            if not data['chinese'] or not data['spanish']:
                break
            chunk = pd.DataFrame(data)
            chunk = chunk[chunk['chinese'].str.strip().astype(bool) & chunk['spanish'].str.strip().astype(bool)]
            yield Dataset.from_pandas(chunk)
            gc.collect()

# Define the path to the test dataset
test_data_path = '/content/UNPC.test.zh-es'
test_dataset = load_data(test_data_path)

# Initialize lists to hold texts
input_texts = []
target_texts = []

for chunk in test_dataset:
    input_texts.extend(chunk['chinese'])
    target_texts.extend(chunk['spanish'])

"""# Load the model"""

def translate_text(text, target_lang='es', source_lang='zh-CN', api_key='AIzaSyC_YZxGXL2NSkXZr9A5HtkXE-pLkn7mhzA'):
    url = "https://translation.googleapis.com/language/translate/v2"
    params = {
        'key': api_key,
        'q': text,
        'source': source_lang,
        'target': target_lang,
        'format': 'text'
    }
    response = requests.get(url, params=params)
    if response.status_code == 200:
        return response.json()['data']['translations'][0]['translatedText']
    else:
        return f"Error: {response.json()['error']['message']}"

google_api_key = 'AIzaSyC_YZxGXL2NSkXZr9A5HtkXE-pLkn7mhzA'

translated_texts = []

for text in input_texts:
    translation = translate_text(text, api_key=google_api_key)
    translated_texts.append(translation)
    print(f"Original: {text} \nTranslated: {translation}\n")

print(f"After translation - Number of translated texts: {len(translated_texts)}")

"""# Evaluation"""

# BLUE evaluation
metric = load_metric("sacrebleu")

# Preprocess the references to match the expected format for sacrebleu
references = [[ref] for ref in target_texts]

# Compute BLEU score
results = metric.compute(predictions=translated_texts, references=references)
print(f"BLEU score: {results['score']}")

! python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/unbabel_comet/wmt20-comet-da/checkpoints/model.ckpt

# COMET evaluation
comet_model = load_from_checkpoint(download_model("wmt20-comet-da"))

comet_data = [{"src": src, "mt": mt, "ref": ref} for src, mt, ref in zip(input_texts, translated_texts, target_texts)]
comet_score = comet_model.predict(comet_data, batch_size=16, gpus=1)
comet_mean_score = np.mean(comet_score['scores'])
print(f"COMET score: {comet_mean_score}")

# Save translations to CSV
df = pd.DataFrame({
    "Source Text": input_texts,
    "Machine Translation": translated_texts,
    "Reference Text": target_texts
})

# Save the DataFrame to a CSV file
df.to_csv("translation_Google.csv", index=False)

print("Data saved to translation_evaluation.csv successfully.")
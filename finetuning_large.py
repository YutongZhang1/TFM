# -*- coding: utf-8 -*-
"""â€œFinetuning_large.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xcBdKzbDlfGIFx6RAGZUR8vLFVgFXkxe
"""

! pip install transformers torch datasets
! pip install transformers[torch]
! pip install accelerate -U
! pip install unbabel-comet sacrebleu
! pip install tqdm

import pandas as pd
import gc
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq
from comet import download_model, load_from_checkpoint
from tqdm.auto import tqdm
import torch
from datasets import Dataset, DatasetDict, concatenate_datasets, load_metric
import numpy as np
import sacrebleu

"""# Preprocessing"""

def load_data(file_path, max_lines=None, chunksize=4000):
    chunks = []
    with open(file_path, 'r', encoding='utf-8') as f:
        total_lines_read = 0
        while True:
            zh_lines = []
            es_lines = []
            for _ in range(chunksize):
                line = f.readline()
                if not line:
                    break
                zh_line, es_line = line.strip().split('\t')
                if max_lines and total_lines_read >= max_lines:
                    break
                zh_lines.append(zh_line.strip())
                es_lines.append(es_line.strip())
                total_lines_read += 1
            if not zh_lines or not es_lines:
                break
            data = {
                'chinese': zh_lines,
                'spanish': es_lines
            }
            chunk = pd.DataFrame(data)
            chunk = chunk[chunk['chinese'].str.strip().astype(bool) & chunk['spanish'].str.strip().astype(bool)]
            chunks.append(Dataset.from_pandas(chunk))
            gc.collect()
    if chunks:
        return concatenate_datasets(chunks)
    else:
        return None

# Define the path to the sets
train_data_path = '/content/UNPC.large.train.zh-es'
validation_data_path = '/content/UNPC.dev.zh-es'
test_data_path = '/content/UNPC.test.zh-es'

# Load datasets
train_dataset = load_data (train_data_path)
validation_dataset = load_data (validation_data_path)
test_dataset = load_data (test_data_path)

# Create a DatasetDict
dataset_dict = DatasetDict({
    'train': train_dataset,
    'validation': validation_dataset,
    'test': test_dataset
})

"""# Train model"""

# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize tokenizer and model
tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh", tgt_lang="es")
model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M").to(device)
model.to(device)

# Tokenize the data
def tokenize_data(batch):
    # Tokenize the input text and the target text
    model_inputs = tokenizer(batch['chinese'], padding="max_length", truncation=True, max_length=128)
    labels = tokenizer(batch['spanish'], padding="max_length", truncation=True, max_length=128)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply tokenization to the datasets
def tokenize_large_dataset(dataset):
    return dataset.map(tokenize_data, batched=True, batch_size=250)

# Tokenize datasets
train_dataset = tokenize_large_dataset(dataset_dict['train'])
dev_dataset = tokenize_large_dataset(dataset_dict['validation'])
test_dataset = tokenize_large_dataset(dataset_dict['test'])

# Set the format of the datasets to PyTorch tensors
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
dev_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

"""# Fine-tuning the model"""

# Define a data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Define training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=3,
    save_steps=2_500,
    logging_steps=1_250,
    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    lr_scheduler_type="linear",
    warmup_steps=2000
)

# Initialize the Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

# Train the model
trainer.train()

"""# Evaluation"""

def translate_texts(dataset, src_language, tokenizer, model, device, batch_size=8, num_beams=5):
    input_texts = dataset[src_language]
    translations = []
    total_batches = len(input_texts) // batch_size + (1 if len(input_texts) % batch_size != 0 else 0)

    for i in tqdm(range(0, len(input_texts), batch_size), total=total_batches, desc="Translating batches"):
        batch_texts = input_texts[i:i + batch_size]
        tokenized_inputs = tokenizer(batch_texts, max_length=128, truncation=True, padding="max_length", return_tensors="pt")
        tokenized_inputs = {k: v.to(device) for k, v in tokenized_inputs.items()}
        with torch.no_grad():
            translated_tokens = model.generate(
                **tokenized_inputs,
                forced_bos_token_id=tokenizer.lang_code_to_id["es"],
                num_beams=num_beams,
                early_stopping=True
            )
        translations.extend(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True))

    return translations

# Generate translations
translated_texts = translate_texts(dataset_dict['test'], 'chinese', tokenizer, model, device, batch_size=8, num_beams=5)

input_texts = test_dataset['chinese']
target_texts = test_dataset['spanish']

# BLUE evaluation
metric = load_metric("sacrebleu")

# Preprocess the references to match the expected format for sacrebleu
references = [[ref] for ref in target_texts]

# Compute BLEU score
results = metric.compute(predictions=translated_texts, references=references)
print(f"BLEU score: {results['score']}")

! python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/unbabel_comet/wmt20-comet-da/checkpoints/model.ckpt

# COMET evaluation
comet_data = [{"src": src, "mt": mt, "ref": ref} for src, mt, ref in zip(input_texts, translated_texts, target_texts)]

comet_model = load_from_checkpoint(download_model("wmt20-comet-da"))

comet_score = comet_model.predict(comet_data, batch_size=16, gpus=1)
comet_mean_score = np.mean(comet_score['scores'])
print(f"COMET score: {comet_mean_score}")

# Save translations to CSV
df = pd.DataFrame({
    "Source Text": input_texts,
    "Machine Translation": translated_texts,
    "Reference Text": target_texts
})

# Save the DataFrame to a CSV file
df.to_csv("finetune_large.csv", index=False)

print("Data saved to translation_evaluation.csv successfully.")